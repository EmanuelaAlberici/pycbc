{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "Collapsed": "false",
        "editable": true,
        "id": "D_QUDECR0psB",
        "tags": []
      },
      "source": [
        "<span style=\"float: left;padding: 1.3em\">![logo](https://github.com/gw-odw/odw/blob/main/Tutorials/logo.png?raw=1)</span>\n",
        "\n",
        "# Gravitational Wave Open Data Workshop\n",
        "\n",
        "## Tutorial 2.1 PyCBC Tutorial, An introduction to matched-filtering\n",
        "\n",
        "We will be using the [PyCBC](https://pycbc.org) library, which is used to study gravitational-wave data, find astrophysical sources due to compact binary mergers, and study their parameters. These are some of the same tools that the LIGO and Virgo collaborations use to find gravitational waves in LIGO/Virgo data\n",
        "\n",
        "In this tutorial we will walk through how to find a specific signal in LIGO data. We present matched filtering as a cross-correlation, in both the time domain and the frequency domain. In the next tutorial (2.2), we use the method as encoded in PyCBC, which is optimal in the case of Gaussian noise and a known signal model. In reality our noise is not entirely Gaussian, and in practice we use a variety of techniques to separate signals from noise in addition to the use of the matched filter.\n",
        "\n",
        "View this tutorial on [Google Colaboratory](https://colab.research.google.com/github/gw-odw/odw/blob/main/Tutorials/Day_2/Tuto_2.1_Matched_filtering_introduction.ipynb) or launch [mybinder](https://mybinder.org/v2/gh/gw-odw/odw/HEAD).\n",
        "\n",
        "See [additional examples](https://pycbc.org/pycbc/latest/html/#library-examples-and-interactive-tutorials) and [documentation](https://pycbc.org/pycbc/latest/html/index.html)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "ZnNr_0C__5iO"
      },
      "outputs": [],
      "source": [
        "# Those 2 lines are just to avoid some harmless warnings when importing packages\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\", \"Wswiglal-redir-stdio\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "Collapsed": "false",
        "editable": true,
        "id": "VfuWmS4z0psH",
        "tags": []
      },
      "source": [
        "## Installation (execute only if running on a cloud platform, like Google Colab, or if you haven't done the installation already!)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "Collapsed": "false",
        "editable": true,
        "id": "bkT6oEIq0psO",
        "tags": []
      },
      "source": [
        "> ⚠️ **Warning**: restart the runtime after running the cell below.\n",
        ">\n",
        "> To do so, click \"Runtime\" in the menu and choose \"Restart and run all\"."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "Collapsed": "false",
        "editable": true,
        "id": "-oaVmOkc0psJ",
        "tags": []
      },
      "outputs": [],
      "source": [
        "# -- Use the following for Google Colab\n",
        "#! pip install -q 'lalsuite==7.25' 'PyCBC==2.4.1'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "Collapsed": "false",
        "editable": true,
        "id": "n1n6Ut_v0psP",
        "tags": []
      },
      "source": [
        "## Matched-filtering: Finding well modelled signals in Gaussian noise\n",
        "\n",
        "Matched filtering can be shown to be the optimal method for \"detecting\" _known_ signals in _Gaussian_ noise. We'll explore those two assumptions a little later, but for now let's demonstrate how this works.\n",
        "\n",
        "Let's assume you have a stretch of noise, white noise to start:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "Collapsed": "false",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 367
        },
        "id": "YUS9v3eM0psQ",
        "outputId": "6c87df33-2dab-46da-e3a5-a2a9dbcb5a87"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "numpy.dtype size changed, may indicate binary incompatibility. Expected 96 from C header, got 88 from PyObject",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-23-d18dbc6bf9cd>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;31m# Generate a long stretch of white noise: the data series and the time series.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msample_rate\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mdata_length\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0mtimes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample_rate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/numpy/__init__.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(attr)\u001b[0m\n\u001b[1;32m    335\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    336\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__dir__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 337\u001b[0;31m         \u001b[0mpublic_symbols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mglobals\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'testing'\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    338\u001b[0m         public_symbols -= {\n\u001b[1;32m    339\u001b[0m             \u001b[0;34m\"core\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"matrixlib\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/numpy/random/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    178\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    179\u001b[0m \u001b[0;31m# add these for module-freeze analysis (like PyInstaller)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 180\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_pickle\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    181\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_common\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    182\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_bounded_integers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/numpy/random/_pickle.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mmtrand\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mRandomState\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_philox\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPhilox\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_pcg64\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPCG64\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mPCG64DXSM\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_sfc64\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSFC64\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32mnumpy/random/mtrand.pyx\u001b[0m in \u001b[0;36minit numpy.random.mtrand\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: numpy.dtype size changed, may indicate binary incompatibility. Expected 96 from C header, got 88 from PyObject"
          ]
        }
      ],
      "source": [
        "import numpy\n",
        "# The first import of matplotlib can take some time (especially on cloud platforms). This is normal.\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# specify the sample rate.\n",
        "# LIGO raw data is sampled at 16384 Hz (=2^14 samples/second).\n",
        "# It captures signal frequency content up to f_Nyquist = 8192 Hz.\n",
        "# Here, we will make the computation faster by sampling at a lower rate.\n",
        "sample_rate = 1024 # samples per second\n",
        "data_length = 1024 # seconds\n",
        "\n",
        "# Generate a long stretch of white noise: the data series and the time series.\n",
        "data = numpy.random.normal(size=[sample_rate * data_length])\n",
        "times = numpy.arange(len(data)) / float(sample_rate)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "Collapsed": "false",
        "id": "Lb1qfZuj0psU"
      },
      "source": [
        "And then let's add a gravitational wave signal to some random part of this data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_6f3YJiQ_5iV"
      },
      "outputs": [],
      "source": [
        "from pycbc.waveform import get_td_waveform\n",
        "\n",
        "# the \"approximant\" (jargon for parameterized waveform family).\n",
        "# IMRPhenomD(a phenomenological Inspiral–Merger–Ringdown wafeform model) is defined in the frequency domain, but we'll get it in the time domain (td).\n",
        "# It runs fast, but it doesn't include effects such as non-aligned component spin, or higher order modes.\n",
        "apx = 'IMRPhenomD'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8QKtRvzG_5iV"
      },
      "source": [
        "Reference for [IMRPhenomD](https://journals.aps.org/prd/abstract/10.1103/PhysRevD.93.044006). You can specify [many parameters](https://pycbc.org/pycbc/latest/html/pycbc.waveform.html?highlight=get_td_waveform#pycbc.waveform.waveform.get_td_waveform), but here, we'll use defaults for everything except the masses.\n",
        "\n",
        "`get_td_waveform` returns both $h_+$ and $h_{\\times}$, but we'll only use $h_+$ for now."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bpIjbzM5_5iW"
      },
      "outputs": [],
      "source": [
        "hp1, _ = get_td_waveform(approximant=apx,\n",
        "                         mass1=10,\n",
        "                         mass2=10,\n",
        "                         delta_t=1.0/sample_rate,\n",
        "                         f_lower=25)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4NG-_ptI_5iW"
      },
      "source": [
        "The amplitude of gravitational-wave signals is normally of order $10^{-20}$. To demonstrate our method on white noise with amplitude $O(1)$ we normalize our signal so the cross-correlation of the signal with itself will give a value of 1.\n",
        "\n",
        "In this case we can interpret the cross-correlation of the signal with white noise as a signal-to-noise ratio."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UgQ6sZbS_5iX"
      },
      "outputs": [],
      "source": [
        "hp1 = hp1 / max(numpy.correlate(hp1, hp1, mode='full'))**0.5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "Collapsed": "false",
        "id": "8iMaIs1d0psW"
      },
      "outputs": [],
      "source": [
        "# note that in this figure, the waveform amplitude is of order 1.\n",
        "# The duration (for frequency above f_lower=25 Hz) is only 3 or 4 seconds long.\n",
        "# The waveform is \"tapered\": slowly ramped up from zero to full strength, over the first second or so.\n",
        "# It is zero-padded at earlier times.\n",
        "plt.figure()\n",
        "plt.title(\"The waveform hp1\")\n",
        "plt.plot(hp1.sample_times, hp1)\n",
        "plt.xlabel('Time (s)')\n",
        "plt.ylabel('Normalized amplitude')\n",
        "\n",
        "# Shift the waveform to start at a random time in the Gaussian noise data.\n",
        "waveform_start = numpy.random.randint(0, len(data) - len(hp1))\n",
        "data[waveform_start:waveform_start+len(hp1)] += 10 * hp1.numpy()\n",
        "\n",
        "plt.figure()\n",
        "plt.title(\"Looks like random noise, right?\")\n",
        "plt.plot(hp1.sample_times, data[waveform_start:waveform_start+len(hp1)])\n",
        "plt.xlabel('Time (s)')\n",
        "plt.ylabel('Normalized amplitude')\n",
        "\n",
        "plt.figure()\n",
        "plt.title(\"Signal in the data\")\n",
        "plt.plot(hp1.sample_times, data[waveform_start:waveform_start+len(hp1)])\n",
        "plt.plot(hp1.sample_times, 10 * hp1)\n",
        "plt.xlabel('Time (s)')\n",
        "plt.ylabel('Normalized amplitude')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "Collapsed": "false",
        "id": "mmAJqGqH0psZ"
      },
      "source": [
        "To search for this signal we can cross-correlate the signal with the entire dataset -> Not in any way optimized at this point, just showing the method.\n",
        "\n",
        "We will do the cross-correlation in the time domain, once for each time step. It runs slowly..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "Collapsed": "false",
        "id": "fi3D6sW70psa"
      },
      "outputs": [],
      "source": [
        "cross_correlation = numpy.zeros([len(data)-len(hp1)])\n",
        "hp1_numpy = hp1.numpy()\n",
        "for i in range(len(data) - len(hp1_numpy)):\n",
        "    cross_correlation[i] = (hp1_numpy * data[i:i+len(hp1_numpy)]).sum()\n",
        "\n",
        "# plot the cross-correlated data vs time. Superimpose the location of the end of the signal;\n",
        "# this is where we should find a peak in the cross-correlation.\n",
        "plt.figure()\n",
        "times = numpy.arange(len(data) - len(hp1_numpy)) / float(sample_rate)\n",
        "plt.plot(times, cross_correlation)\n",
        "plt.plot([waveform_start/float(sample_rate), waveform_start/float(sample_rate)], [-10,10],'r:')\n",
        "plt.xlabel('Time (s)')\n",
        "plt.ylabel('Cross-correlation')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q-cRdmNS_5ib"
      },
      "source": [
        "### Detection in Colored Noise"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "Collapsed": "false",
        "id": "cafBZG960psc"
      },
      "source": [
        "Here you can see that the largest spike from the cross-correlation comes at the time of the signal. We only really need one more ingredient to describe matched-filtering: \"Colored\" noise (Gaussian noise but with a frequency-dependent variance; white noise has frequency-independent variance).\n",
        "\n",
        "Let's repeat the process, but generate a stretch of data colored with LIGO's zero-detuned--high-power noise curve. We'll use the PyCBC library to do this."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "Collapsed": "false",
        "id": "9Jcw-P_v0psd"
      },
      "outputs": [],
      "source": [
        "# https://pycbc.org/pycbc/latest/html/noise.html\n",
        "import pycbc.noise\n",
        "import pycbc.psd\n",
        "\n",
        "# The color of the noise matches a PSD which you provide:\n",
        "# Generate a PSD matching Advanced LIGO's zero-detuned--high-power noise curve\n",
        "flow = 10.0\n",
        "delta_f = 1.0 / 128\n",
        "flen = int(sample_rate / (2 * delta_f)) + 1\n",
        "psd = pycbc.psd.aLIGOZeroDetHighPower(flen, delta_f, flow)\n",
        "\n",
        "# Generate colored noise\n",
        "delta_t = 1.0 / sample_rate\n",
        "ts = pycbc.noise.noise_from_psd(data_length*sample_rate, delta_t, psd, seed=127)\n",
        "\n",
        "# Estimate the amplitude spectral density (ASD = sqrt(PSD)) for the noisy data\n",
        "# using the \"welch\" method. We'll choose 4 seconds PSD samples that are overlapped 50%\n",
        "seg_len = int(4 / delta_t)\n",
        "seg_stride = int(seg_len / 2)\n",
        "estimated_psd = pycbc.psd.welch(ts,seg_len=seg_len,seg_stride=seg_stride)\n",
        "\n",
        "# plot it:\n",
        "plt.loglog(estimated_psd.sample_frequencies, estimated_psd, label='estimate')\n",
        "plt.loglog(psd.sample_frequencies, psd, linewidth=3, label='known psd')\n",
        "plt.xlim(xmin=flow, xmax=512)\n",
        "plt.ylim(1e-47, 1e-45)\n",
        "plt.xlabel('Frequency [Hz]')\n",
        "plt.ylabel('Power spectral density')\n",
        "plt.legend()\n",
        "plt.grid()\n",
        "plt.show()\n",
        "\n",
        "# add the signal, this time, with a \"typical\" amplitude.\n",
        "ts[waveform_start:waveform_start+len(hp1)] += hp1.numpy() * 1E-20"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "Collapsed": "false",
        "id": "rSgmyob_0psf"
      },
      "source": [
        "Then all we need to do is to \"whiten\" both the data, and the template waveform. This can be done, in the frequency domain, by dividing by the PSD. This *can* be done in the time domain as well, but it's more intuitive in the frequency domain"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "Collapsed": "false",
        "id": "w_v6gzUh0psg"
      },
      "outputs": [],
      "source": [
        "# Generate a PSD for whitening the data\n",
        "from pycbc.types import TimeSeries\n",
        "\n",
        "# The PSD, sampled properly for the noisy data\n",
        "flow = 10.0\n",
        "delta_f = 1.0 / data_length\n",
        "flen = int(sample_rate / (2 * delta_f)) + 1\n",
        "psd_td = pycbc.psd.aLIGOZeroDetHighPower(flen, delta_f, 0)\n",
        "\n",
        "# The PSD, sampled properly for the signal\n",
        "delta_f = sample_rate / float(len(hp1))\n",
        "flen = int(sample_rate / (2 * delta_f)) + 1\n",
        "psd_hp1 = pycbc.psd.aLIGOZeroDetHighPower(flen, delta_f, 0)\n",
        "\n",
        "# The 0th and Nth values are zero. Set them to a nearby value to avoid dividing by zero.\n",
        "psd_td[0] = psd_td[1]\n",
        "psd_td[len(psd_td) - 1] = psd_td[len(psd_td) - 2]\n",
        "# Same, for the PSD sampled for the signal\n",
        "psd_hp1[0] = psd_hp1[1]\n",
        "psd_hp1[len(psd_hp1) - 1] = psd_hp1[len(psd_hp1) - 2]\n",
        "\n",
        "# convert both noisy data and the signal to frequency domain,\n",
        "# and divide each by ASD=PSD**0.5, then convert back to time domain.\n",
        "# This \"whitens\" the data and the signal template.\n",
        "# Multiplying the signal template by 1E-21 puts it into realistic units of strain.\n",
        "data_whitened = (ts.to_frequencyseries() / psd_td**0.5).to_timeseries()\n",
        "hp1_whitened = (hp1.to_frequencyseries() / psd_hp1**0.5).to_timeseries() * 1E-21"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "Collapsed": "false",
        "editable": true,
        "id": "d_HLbCCN0psi",
        "tags": []
      },
      "outputs": [],
      "source": [
        "# Now let's re-do the correlation, in the time domain, but with whitened data and template.\n",
        "cross_correlation = numpy.zeros([len(data)-len(hp1)])\n",
        "hp1n = hp1_whitened.numpy()\n",
        "datan = data_whitened.numpy()\n",
        "for i in range(len(datan) - len(hp1n)):\n",
        "    cross_correlation[i] = (hp1n * datan[i:i+len(hp1n)]).sum()\n",
        "\n",
        "# plot the cross-correlation in the time domain. Superimpose the location of the end of the signal.\n",
        "# Note how much bigger the cross-correlation peak is, relative to the noise level,\n",
        "# compared with the unwhitened version of the same quantity. SNR is much higher!\n",
        "plt.figure()\n",
        "times = numpy.arange(len(datan) - len(hp1n)) / float(sample_rate)\n",
        "plt.plot(times, cross_correlation)\n",
        "plt.plot([waveform_start/float(sample_rate), waveform_start/float(sample_rate)],\n",
        "           [(min(cross_correlation))*1.1,(max(cross_correlation))*1.1],'r:')\n",
        "plt.xlabel('Time (s)')\n",
        "plt.ylabel('Cross-correlation')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jLzKDXhX_5if"
      },
      "source": [
        "# Challenge!\n",
        "\n",
        "* Histogram the whitened time series. Ignoring the outliers associated with the signal, is it a Gaussian? What is the mean and standard deviation? (We have not been careful in normalizing the whitened data properly).\n",
        "* Histogram the above cross-correlation time series. Ignoring the outliers associated with the signal, is it a Gaussian? What is the mean and standard deviation?\n",
        "* (Optional) Find the location of the peak. (Note that here, it can be positive or negative), and the value of the SNR of the signal (which is the absolute value of the peak value, divided by the standard deviation of the cross-correlation time series)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "editable": true,
        "tags": [],
        "id": "bA3mr5su_5if"
      },
      "outputs": [],
      "source": [
        "datastd = data_whitened.data.std()\n",
        "pylab.hist(data_whitened.data, bins=numpy.arange(-6,6,0.2)*datastd)\n",
        "print('data_whitened.data.std =',datastd)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "Tuto_2.1_Matched_filtering_introduction.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}